es学习日志

RestClient API

<Elasticsearch权威指南> : https://www.elastic.co/guide/cn/elasticsearch/guide/current/intro.html

<Elasticsearch 官方文档> : https://www.elastic.co/guide/en/elasticsearch/reference/current/_basic_concepts.html

1、基础知识
安装

2、映射基础
text:支持分词索引，支持模糊（match）&精确查询，不支持聚合。一般用于全文搜索，比如博客内容检索，在检索前需要对内容进行分词操作，不能用该字段进行排序，一般不用来做聚合操作。
keyword:不支持分词索引，支持模糊（match）&精确查询，支持聚合。一般用来索引结构化的内容(如邮箱地址，电话号码，状态，性别)。一般用来进行状态过滤、聚合、排序操作。

3、搜索基础

精确值查找
当进行精确值查找的时候通常使用filter(过滤器)，filter的执行速度非常快(不需要进行评分，在数据量一定的范围内容易被缓存在内存中)。想要提高性能，应该尽可能多的使用过滤器。
--查询流程为：
1.term在倒排索引中获取到符合条件的文档有三个。
2.创建位图(bitset，只包含0和1，1代表文档符合过滤器条件，0则代表不符合)。关于bitset想要了解更多可以点击：roaring bitmap
3.迭代bitsets从而获取到符合条件的文档集合，迭代会优先迭代稀疏的bitset(可以排除掉大量不符合条件的文档)
4.es会保存下每次查询的使用次数。如果查询在最近的 256 次查询中会被用到，那么它就会被缓存到内存中。当 bitset 被缓存后，注: 低于 10,000 个文档（或少于 3% 的总索引数）的段（segment）不会被缓存。
5.补充：一旦缓存成功了，那么bitset将会一只驻留在内存中知道它被剔除。剔除的规则是基于LRU算法(最近最少使用)来实现

全文检索&分析器
相关性：评价结果和查询条件的匹配程度，并根据相关性作为默认的排序条件.
	检索词频率：检索词在该文档中出现的频率（出现的次数越多，相关性越高）
	反向文档频率：检索词在该索引上所有文档中出现的频率（出现的次数越多，相关性越低）
	字段长度准则：在文档中，字段的内容的长度（字段越长，相关性越低）
分析：将文本转换为有区别的、规范化的一个token的过程。目的就是为了创建倒排索引。
倒排索引：适用于全文搜索的数据结构。有文档中不重复的词的列表构成，对于每一个不重复的词，都一个列表，这个列表即为包含该词的文档。
分析的流程：将一块文本分解成适合于倒排索引的词条，然后将这些词条统一格式化，从而提高可搜索性。
	字符过滤器：在分词前整理字符串。比如，可以用来去掉HTML，或者将 & 转化成 `and`。
	分词器：将字符串分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。
	Token过滤器：该过滤器可能会改变词条（例如，将大些字母转换为小写），删除词条（像` a`， `and`， `the` 等无用词），或者增加词条（像 jump 和 leap 这种同义词）


4、分页
from/size:原理类似于mysql的limit，查询的数据越往后，效率越低
scroll:原理类似于消息消费机制，首次查询会保存一个历史快照及游标（scroll_id），记录当前消息查询的终止位置，下次查询的时候将基于游标进行消费(性能良好，维护成本高，在游标失效前，不会更新数据，不够灵活，一旦游标创建size就不可改变，适用于大量数据导出或者索引重建)
search_after:性能优秀，类似于优化后的分页查询，历史条件过滤掉数据

总结：在分页的场景中，不建议直接使用scroll，scroll无法保证数据实时更新，数据量较小而有随机分页的场景可以使用from/size的方式分页，而在大多数场景中推荐使用search_after。

5、层级文档
理解为mysql多表关联

6、聚合基础


es的setting和mapping例子：
setting:
{
  "cust_2016090519205300000": {
    "settings": {
      "index": {
        "number_of_shards": "5",
        "provided_name": "cust_2016090519205300000",
        "max_result_window": "1000000000",
        "creation_date": "1519362414487",
        "analysis": { // 自定义分词器
          "analyzer": { // 关键字
            "my_ngram_analyzer": { // 自定义的分词器
              "character filters": "",  // 针对原始文本进行处理，比如去除html标签
              "tokenizer": "my_ngram_tokenizer",  // 将原始文本按照一定规则切分为单词
              "token filters": "",  // 针对Tokenizer处理的单词进行再加工，比如转小写、删除或增新等处理
            },
            "my_analyzer":{ // 自定义的分词器
              "type":"standard",  // 分词器类型standard
              "stopwords":"_english_" // standard分词器的参数，默认的stopwords是\_none_
            }
          },
          "tokenizer": {
            "my_ngram_tokenizer": { // 其中my_ngram_tokenizer 用于设置最小分词、最大分词，该分词器用于处理 无法精确查询中文字段；
              "token_chars": [
                "letter",
                "digit"
              ],
              "min_gram": "2",
              "type": "nGram",
              "max_gram": "3"
            }
          }
        },
        "number_of_replicas": "1",
        "uuid": "ERntEIMET8a0iwlT7paYjA",
        "version": {
          "created": "5060199"
        }
      }
    }
  }
}

mapping:
{
  "cust_2016090519205300000": {
    "mappings": {
      "customer_type": {   // customer_type 为索引类型，在6.0之后的版本该类型每个索引只能又一个,尝试创建第二个类型的时候es报错
        "dynamic": "strict", // dynamic取值，true：默认值，动态添加字段；false：忽略新字段；strict：碰到陌生字段，抛出异常。
        "_source": {    // 如果有的字段只想索引，不想存储，可以使用_source
          "excludes": ["AesPhoneNum", "AesEmail"]
        },
        "properties": {
          "ClassID": {
            "type": "text", // text类型支持分词索引，支持模糊查询、精确查询，不支持聚合。一般用于全文搜索，比如博客内容检索，在检索前要对内容进行分词操作
                            // 不能对该字段进行排序，一般不用来聚合
            "fields": {     // ClassID的内置属性，可以用ClassID.keyword访问
              "keyword": {
                "type": "keyword",  // keyword类型不支持分词索引，但支持模糊查询、精确查询、聚合等操作，一般用来索引结构化内容，如邮箱地址，电话号码，状态，性别）
                                    // 一般用来进行状态过滤、聚合、排序
                "ignore_above": 256,
                "index_options": "freqs" // index_options参数用于控制增加到倒排索引的信息，为了搜索和高亮，接受如下设置：
                                         // docs 只索引文档号，可用于回答词项是否存在于文档中的这个域
                                         // freqs 文档号和词频都会被存储，词项频率越高积分越高
                                         // positions 文档号、词项、词的位置都会被索引，位置可以用于模糊或短语查询
                                         // offsets 文档号、词项、词的位置、开始到结束的字符偏移（词项映射到原来的字符串）被索引，偏移提供postings highlighter
                                         // 分析字符串域默认是会使用positions，其他域默认使用docs。
              }
            }
          },
          "attr1517905675669": {
            "type": "long"
          },
          "birthday": {
            "type": "date",
            "format": "yyyy-MM-dd HH:mm:ss"
          },
          "city": {
            "type": "text",
            "fields": {
              "id": {
                "type": "keyword",
                "index": false
              }
            },
            "index":"analyzed", // 该字段要被分词索引
            "analyzer": "ik_smart", // 存储时，使用的分词器
            "search_analyzer": "ik_max_word", // 查询时，使用的分词器
            "fielddata": true
          },
          "createTime": {
            "type": "date",
            "format": "yyyy-MM-dd HH:mm:ss"
          },
          "custName": {
            "type": "text",
            "boost": 8,
            "store": true,  // 是否存储, 适合all
            "fields": {
              "keyword": {
                "type": "text"
              }
            },
            "analyzer": "my_ngram_analyzer",
            "include_in_all": true,
            "fielddata": true
          },
          "language": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          },
          "mobile": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            },
            "analyzer": "ik_max_word",
            "fielddata": true
          },
          "province": {
            "type": "text",
            "fields": {
              "id": {
                "type": "keyword",
                "index": false
              }
            },
            "analyzer": "ik_smart",
            "fielddata": true
          },
          "tagCount": {
            "type": "nested",
            "properties": {
              "11": {
                "type": "long"
              },
              "12": {
                "type": "long"
              }
            }
          }
        }
      }
    }
  }
}

----对于有数据的需要更改的mapping
POST /my_index/_close
PUT /my_source_index/_mappings
POST /my_index/_open
这样可以保证原数据不丢, 但执行过程中会丢掉执行过程的1-2s的数据